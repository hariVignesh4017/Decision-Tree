{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d9a8aa",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086b79f",
   "metadata": {},
   "source": [
    "**Q: What is a Decision Tree, and how does it work**\n",
    "\n",
    "A: A Decision Tree is a supervised learning model used for classification and regression tasks. It splits data into branches based on feature values, creating a tree-like model of decisions. At each node, it selects the feature that best separates the data using criteria like Gini Impurity or Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0e6e3",
   "metadata": {},
   "source": [
    "**Q: What are impurity measures in Decision Trees**\n",
    "\n",
    "A: Impurity measures are metrics used to evaluate the quality of a split at a node in a Decision Tree. Common impurity measures include Gini Impurity and Entropy. Lower impurity means better class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e8a57",
   "metadata": {},
   "source": [
    "**Q: What is the mathematical formula for Gini Impurity**\n",
    "\n",
    "A: Gini Impurity = 1 - Σ (p_i)^2, where p_i is the probability of class i at a particular node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2a3d1",
   "metadata": {},
   "source": [
    "**Q: What is the mathematical formula for Entropy**\n",
    "\n",
    "A: Entropy = -Σ p_i * log2(p_i), where p_i is the probability of class i at a particular node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7ea9c",
   "metadata": {},
   "source": [
    "**Q: What is Information Gain, and how is it used in Decision Trees**\n",
    "\n",
    "A: Information Gain measures the reduction in impurity or entropy after a dataset is split on a feature. It helps the Decision Tree algorithm choose the best feature for splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31599a67",
   "metadata": {},
   "source": [
    "**Q: What is the difference between Gini Impurity and Entropy**\n",
    "\n",
    "A: Both measure impurity, but Gini is simpler and faster to compute, while Entropy uses logarithms and is more information-theoretic. They often yield similar results in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40196c58",
   "metadata": {},
   "source": [
    "**Q: What is the mathematical explanation behind Decision Trees**\n",
    "\n",
    "A: Decision Trees use recursive binary splitting to partition the data space. At each node, a feature and a threshold are chosen to maximize Information Gain or minimize Gini Impurity. This process continues until a stopping condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0a836",
   "metadata": {},
   "source": [
    "**Q: What is Pre-Pruning in Decision Trees**\n",
    "\n",
    "A: Pre-pruning stops the tree from growing once a condition is met (like max depth or min samples), helping to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aad5cf",
   "metadata": {},
   "source": [
    "**Q: What is Post-Pruning in Decision Trees**\n",
    "\n",
    "A: Post-pruning builds a full tree first and then removes branches that do not improve performance on validation data, improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcdb415",
   "metadata": {},
   "source": [
    "**Q: What is the difference between Pre-Pruning and Post-Pruning**\n",
    "\n",
    "A: Pre-pruning halts tree growth early, while post-pruning trims a fully grown tree. Pre-pruning avoids overfitting upfront, whereas post-pruning corrects it after full growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e999e92",
   "metadata": {},
   "source": [
    "**Q: What is a Decision Tree Regressor**\n",
    "\n",
    "A: A Decision Tree Regressor predicts continuous values by partitioning the data and fitting simple models (e.g., mean) in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280b52b",
   "metadata": {},
   "source": [
    "**Q: What are the advantages and disadvantages of Decision Trees**\n",
    "\n",
    "A: Advantages: easy to interpret, handle both numerical and categorical data, require little preprocessing. Disadvantages: prone to overfitting, unstable with small variations in data, can be biased if classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d108c6",
   "metadata": {},
   "source": [
    "**Q: How does a Decision Tree handle missing values**\n",
    "\n",
    "A: Some implementations handle missing values by surrogate splits or assigning instances based on distribution. Others require preprocessing to fill or drop missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b2465",
   "metadata": {},
   "source": [
    "**Q: How does a Decision Tree handle categorical features**\n",
    "\n",
    "A: Categorical features are handled by splitting nodes based on the category values. Many libraries automatically encode them, while others require manual preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae29c6",
   "metadata": {},
   "source": [
    "**Q: What are some real-world applications of Decision Trees?**\n",
    "\n",
    "A: Applications include medical diagnosis, credit scoring, fraud detection, customer churn prediction, and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b99791",
   "metadata": {},
   "source": [
    "## Practical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9103446",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b90e99",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Feature Importances:\", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f0201",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbbbe3",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173fabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5306169838965117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.3, random_state=42)\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56207a2",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9072947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names,\n",
    "                           class_names=iris.target_names, filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"decision_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12f063",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e24ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with max_depth=3: 1.0000\n",
      "Accuracy with fully grown tree: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset and split it into training and testing sets\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train with max_depth=3\n",
    "clf_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_depth_3.fit(X_train, y_train)\n",
    "y_pred_depth_3 = clf_depth_3.predict(X_test)\n",
    "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
    "\n",
    "# Train with no depth limit (fully grown tree)\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "print(f\"Accuracy with max_depth=3: {accuracy_depth_3:.4f}\")\n",
    "print(f\"Accuracy with fully grown tree: {accuracy_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b44b8f",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_split_5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
    "clf_split_5.fit(X_train, y_train)\n",
    "y_pred_split_5 = clf_split_5.predict(X_test)\n",
    "accuracy_split_5 = accuracy_score(y_test, y_pred_split_5)\n",
    "\n",
    "# Train with default tree\n",
    "clf_default = DecisionTreeClassifier(random_state=42)\n",
    "clf_default.fit(X_train, y_train)\n",
    "y_pred_default = clf_default.predict(X_test)\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "\n",
    "print(f\"Accuracy with min_samples_split=5: {accuracy_split_5:.4f}\")\n",
    "print(f\"Accuracy with default tree: {accuracy_default:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e521c",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train on scaled data\n",
    "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
    "clf_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Train on unscaled data\n",
    "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
    "clf_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "print(f\"Accuracy with scaled data: {accuracy_scaled:.4f}\")\n",
    "print(f\"Accuracy with unscaled data: {accuracy_unscaled:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7621a",
   "metadata": {},
   "source": [
    " **Q: Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ea0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Train using One-vs-Rest strategy\n",
    "clf_ovr = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
    "clf_ovr.fit(X_train, y_train)\n",
    "y_pred_ovr = clf_ovr.predict(X_test)\n",
    "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "\n",
    "print(f\"Accuracy with One-vs-Rest strategy: {accuracy_ovr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf56731",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier and display the feature importance scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87813d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier\n",
    "clf_importance = DecisionTreeClassifier(random_state=42)\n",
    "clf_importance.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = clf_importance.feature_importances_\n",
    "\n",
    "print(f\"Feature importance scores: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40779116",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d22d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with max_depth=5: 0.0000\n",
      "MSE with unrestricted tree: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a regression problem (e.g., predict the target variable using a subset of features)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train with max_depth=5\n",
    "regressor_depth_5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "regressor_depth_5.fit(X_train_reg, y_train_reg)\n",
    "y_pred_depth_5 = regressor_depth_5.predict(X_test_reg)\n",
    "mse_depth_5 = mean_squared_error(y_test_reg, y_pred_depth_5)\n",
    "\n",
    "# Train with unrestricted tree\n",
    "regressor_full = DecisionTreeRegressor(random_state=42)\n",
    "regressor_full.fit(X_train_reg, y_train_reg)\n",
    "y_pred_full_reg = regressor_full.predict(X_test_reg)\n",
    "mse_full = mean_squared_error(y_test_reg, y_pred_full_reg)\n",
    "\n",
    "print(f\"MSE with max_depth=5: {mse_depth_5:.4f}\")\n",
    "print(f\"MSE with unrestricted tree: {mse_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40e8c4",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf_ccp = DecisionTreeClassifier(random_state=42)\n",
    "clf_ccp.fit(X_train, y_train)\n",
    "\n",
    "# Get the effective alpha values for pruning\n",
    "path = clf_ccp.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Plot the effects of pruning on the training accuracy\n",
    "train_scores = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Plot the pruning curve\n",
    "plt.plot(ccp_alphas, train_scores, marker='o')\n",
    "plt.xlabel('Alpha (Pruning Parameter)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Cost Complexity Pruning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989b95",
   "metadata": {},
   "source": [
    "**Q:Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf_pr = DecisionTreeClassifier(random_state=42)\n",
    "clf_pr.fit(X_train, y_train)\n",
    "y_pred_pr = clf_pr.predict(X_test)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred_pr, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_pr, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_pr, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12d4f5",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf_cm = DecisionTreeClassifier(random_state=42)\n",
    "clf_cm.fit(X_train, y_train)\n",
    "y_pred_cm = clf_cm.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_cm)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bd733",
   "metadata": {},
   "source": [
    "**Q: Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf_grid = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf_grid, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
